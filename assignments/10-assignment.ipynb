{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Parallel Computing\n",
    "\n",
    "## Due 27 June 2025\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This assignment is about parallel computing with Dask. You should use Python to implement the calculations. If possible, please submit your answers in PDF or HTML format. Also, please use the virtual environment we created in class to avoid dependency issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the concept of \"overhead\" in parallel computing with `joblib`. Why might running a very simple task (like adding 1 to a number) in parallel with `joblib` be slower than running it serially?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overhead in parallel computing pertains to the processes needed to set up parallel computing. Parallel computing may be faster in more computationally intensive tasks as the overhead time only represents a small fraction of the total time required to run the whole code. However, in simpler tasks, overhead now constitutes a larger fraction of the total time relative to the time it takes to perform the computation itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a Python function `count_vowels(text)` that counts the vowels (a, e, i, o, u, case-insensitive) in a given string. Then, use the `Parallel` and `delayed` functions from the `joblib` library to apply your function in parallel. Use all available cores.  The function should return a list of integers, where each integer corresponds to the number of vowels in the respective sentence.\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    \"Joblib makes parallel computing easy\",\n",
    "    \"Dask scales Python code effectively\",\n",
    "    \"Parallelism can speed up computations\",\n",
    "    \"Always consider the overhead\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 10, 13, 10]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your answer here\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sentences = [\n",
    "    \"Joblib makes parallel computing easy\",\n",
    "    \"Dask scales Python code effectively\",\n",
    "    \"Parallelism can speed up computations\",\n",
    "    \"Always consider the overhead\"\n",
    "]\n",
    "\n",
    "def count_vowels(text):\n",
    "    count = 0\n",
    "    for i in range(0, len(text)):\n",
    "        if text[i] in [\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"]:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "Parallel(n_jobs=6)(delayed(count_vowels)(sentence) for sentence in sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function called `get_length` that takes a word as input and returns its length. Then, using the provided list `words`, do the following:\n",
    "\n",
    "* Use a standard (sequential) for loop to calculate the length of each word by calling your function.\n",
    "* Use the `joblib` library to calculate the length of each word in parallel, also calling your function. Use `Parallel` and `delayed` from `joblib` again.\n",
    "* Compare the syntax of the sequential and parallel approaches. How do they differ when writing the loop?\n",
    "\n",
    "```python\n",
    "words = [\"joblib\", \"parallel\", \"computing\", \"example\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 9, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your answer here\n",
    "\n",
    "words = [\"joblib\", \"parallel\", \"computing\", \"example\"]\n",
    "\n",
    "def get_length(word):\n",
    "    return len(word)\n",
    "\n",
    "for word in words:\n",
    "    get_length(word)\n",
    "\n",
    "Parallel(n_jobs=6)(delayed(get_length)(word) for word in words)\n",
    "\n",
    "# Though it uses 2 lines, the sequential code is easier to read. The parallel code, while only being 1 line, has more parameters and is harder to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a 10000x10000 Dask array `da_a` filled with random integers between 0 and 100, chunked into (500, 1000) blocks. Use `RandomState(350)` to make your code reproducible. Create a second Dask array `da_b` of the same shape and chunks, filled with ones. Compute `da_c = (da_a + da_b) * 2` and its mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.99471242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your answer here\n",
    "\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "seed = np.random.RandomState(350)\n",
    "numbers = seed.randint(100, size = (10000,10000))\n",
    "da_a = da.from_array(numbers, chunks=(500,1000))\n",
    "\n",
    "ones = np.full((10000,10000), 1)\n",
    "da_b = da.from_array(ones, chunks=(500,1000))\n",
    "\n",
    "da_c = (da_a + da_b) * 2\n",
    "da_c.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the difference between `dask.dataframe.compute()` and `dask.dataframe.persist()`? When would you typically use `.persist()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here.\n",
    "\n",
    "Both `dask.dataframe.compute()` and `dask.dataframe.persist()` can be used to make a computation on a dask dataframe. However, `.compute()` merely returns the computation while `.persist()` keeps the result of the computation within memory, which is akin to creating a new variable as it can be useful if you want to reference that result later instead of having to make the entire computation again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In this question, you will compare the performance of a regular `for` loop and `dask` for a simple computation. First, create a function called `intensive_task` as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import time\n",
    "import dask\n",
    "\n",
    "def intensive_task(n):\n",
    "    loop_limit = 10_000_000 # How many iterations inside the function\n",
    "    total = 0\n",
    "    for i in range(loop_limit):\n",
    "        total += i*i\n",
    "    return total\n",
    "```\n",
    "\n",
    "Then, create a list called `inputs` with 6 values:\n",
    "\n",
    "```python\n",
    "inputs = [1, 2, 3, 4, 5, 6] \n",
    "```\n",
    "\n",
    "Now, use the function `time.time()` to measure the time it takes to run the function `intensive_task` for each value in the list `inputs` using a regular `for` loop. Store the results in a list called `results`. Remember to create the `start_time` and `end_time` variables to measure the time taken for the computation. The result, which is the difference between `end_time` and `start_time`, should be printed.\n",
    "\n",
    "Repeat the same task using `dask`. However, instead of using the `@dask.delayed` decorator, use the code below:\n",
    "\n",
    "```python\n",
    "tasks = [dask.delayed(intensive_task)(i) for i in inputs]\n",
    "```\n",
    "\n",
    "Then, use `dask.compute()` to compute the results. Again, measure the time taken for the computation and print the result. Which one is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6655237674713135\n",
      "2.662086248397827\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import dask\n",
    "\n",
    "def intensive_task(n):\n",
    "    loop_limit = 10_000_000 # How many iterations inside the function\n",
    "    total = 0\n",
    "    for i in range(loop_limit):\n",
    "        total += i*i\n",
    "    return total\n",
    "\n",
    "inputs = [1, 2, 3, 4, 5, 6] \n",
    "\n",
    "start = time.time()\n",
    "results = []\n",
    "for i in inputs:\n",
    "    results.append(intensive_task(i))\n",
    "print(time.time()-start)\n",
    "\n",
    "start2 = time.time()\n",
    "dask.compute([dask.delayed(intensive_task)(i) for i in inputs])\n",
    "print(time.time()-start2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. In the same folder as this notebook, you will find a Parquet file named `data.parquet`. It is available here: <https://github.com/danilofreire/qtm350-summer/blob/main/assignments/data.parquet>. This file contains student records with the following columns:\n",
    "\n",
    "* `emory_id` (integer) \n",
    "* `student_name` (string)\n",
    "* `major` (string)\n",
    "* `gpa` (float)\n",
    "\n",
    "Write Python code using `dask.dataframe` to read the `data.parquet` file, but only load the `major` and `gpa` columns. Then, print the first 5 rows of the resulting Dask DataFrame using the `.head()` method, and calculate the average GPA by major.\n",
    "\n",
    "You will need a Parquet engine to read the file. If you don't have one installed, you can use `pyarrow`. You can install it using conda (or pip):\n",
    "\n",
    "```bash\n",
    "conda install pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       major   gpa\n",
      "0    History  2.98\n",
      "1  Chemistry  3.16\n",
      "2  Chemistry  3.83\n",
      "3        QTM  3.67\n",
      "4    CompSci  3.33\n",
      "            avg_gpa\n",
      "major              \n",
      "History    3.098750\n",
      "Chemistry  3.320000\n",
      "QTM        2.957500\n",
      "CompSci    3.352857\n",
      "Biology    3.044000\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "import dask\n",
    "\n",
    "data = dask.dataframe.read_parquet('data.parquet', columns = (\"major\", \"gpa\"))\n",
    "print(data.head())\n",
    "data_grouped = data.groupby(\"major\").agg(avg_gpa=(\"gpa\", \"mean\")).compute()\n",
    "print(data_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. You have two CSV files in this directory:\n",
    "\n",
    "* `students.csv`: Contains columns `student_id`, `student_name`. Available here: <https://github.com/danilofreire/qtm350-summer/blob/main/assignments/students.csv>.\n",
    "* `grades.csv`: Contains columns `student_id`, `course`, `grade`. Available here: <https://github.com/danilofreire/qtm350-summer/blob/main/assignments/grades.csv>.\n",
    "\n",
    "Write Python code using dask.dataframe to:\n",
    "\n",
    "* Read `students.csv` into a Dask DataFrame called `ddf_students`.\n",
    "* Read `grades.csv` into a Dask DataFrame called `ddf_grades`.\n",
    "* Merge these two DataFrames together based on the common `student_id` column. An inner merge is recommended (only include students present in both files).\n",
    "* From the merged DataFrame, select only the `student_name`, `course`, and `grade` columns. Save it as `ddf_final`.\n",
    "* Compute and print the first 5 rows of this final merged DataFrame using `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_name</th>\n",
       "      <th>course</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>QTM100</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>QTM200</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td>QTM100</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob</td>\n",
       "      <td>QTM300</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>QTM100</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  student_name  course grade\n",
       "0        Alice  QTM100     A\n",
       "1        Alice  QTM200     B\n",
       "2          Bob  QTM100     B\n",
       "3          Bob  QTM300     C\n",
       "4      Charlie  QTM100     A"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_sql import Context\n",
    "\n",
    "ddf_students = dd.read_csv(\"students.csv\")\n",
    "ddf_grades = dd.read_csv(\"grades.csv\")\n",
    "merged = ddf_students.merge(ddf_grades, on='student_id', how='inner').compute()\n",
    "ddf_final = merged[['student_name','course','grade']]\n",
    "ddf_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck! 😃"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
