{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTM 350 - Data Science Computing\n",
    "## Practice: Interactive Data Analysis with Jupyter on AWS EC2\n",
    "**Author:** Danilo Freire (danilo.freire@emory.edu, Emory University)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Practice: The Goal\n",
    "\n",
    "- **Objective:**\n",
    "- In this comprehensive session, you will use a remote cloud server as a fully interactive data science workbench. This is a common and powerful workflow for handling data analysis tasks.\n",
    "\n",
    "- **Process:**\n",
    "- You will launch an EC2 instance, set up a remote Jupyter Notebook environment using SSH port forwarding, download and clean a dataset interactively, perform an analysis with visualisation, and then retrieve your final work (both the notebook and the cleaned data).\n",
    "\n",
    "- **Environment:**\n",
    "- This workflow uses a single AWS EC2 instance managed from your local terminal.\n",
    "\n",
    "- **Estimated Time:**\n",
    "- 60-75 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Server Setup and Jupyter Launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Launch an EC2 Instance:**\n",
    "- Log into the AWS Console and launch one `t2.micro` instance. Use the `Ubuntu Server 24.04 LTS` image and name it `jupyter-ec2-instance`.\n",
    "- Use your existing `qtm350_key.pem` key pair and configure the security group to allow SSH traffic from your IP address.\n",
    "\n",
    "- **Connect and Install Software:**\n",
    "- Connect to your instance using `ssh`.\n",
    "- Once connected, install wget, `pandas`, `matplotlib`, `seaborn`, and Jupyter on the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Start a Port-Forwarding SSH Session:**\n",
    "- Open a **new local terminal window** (keep your original SSH session open for now) and run the following command. This maps port 8000 on your local machine to port 8888 on the EC2 instance, allowing you to access the remote Jupyter server.\n",
    "\n",
    "- **Start the Jupyter Notebook Server:**\n",
    "- In your **original SSH terminal** (the one already connected to EC2), start the Jupyter server. The `--no-browser` flag is important.\n",
    "\n",
    "- Jupyter will print a URL containing a security token. Copy this token (the long string of characters after `token=`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Access Jupyter in Your Browser:**\n",
    "- Open a web browser on your **local machine** and navigate to `http://localhost:8000`.\n",
    "- Paste the token you copied into the password box to log in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Interactive Data Cleaning and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now running a Jupyter Notebook session on a remote cloud server. All work from this point will be done in notebook cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Create a New Notebook:**\n",
    "- In the Jupyter file browser, click `New` -> `Python 3 (ipykernel)` to create a new notebook. Rename it `employee_analysis.ipynb`.\n",
    "\n",
    "- **Download the Raw Data (Inside the Notebook):**\n",
    "- In the first cell of your new notebook, use the `wget` command to download the `dirty_employee_data` to your EC2 instance.. More information about the command [here](https://niagads.scrollhelp.site/support/wget-linux-file-downloader-user-guide). \n",
    "- The link is <https://github.com/danilofreire/qtm350-summer/blob/main/lectures/lecture-20/dirty_employee_data.csv>.\n",
    "- If you need to create the dataset, you can find the script here: <https://github.com/danilofreire/qtm350-summer/blob/main/lecture-20/dirty-data-generation.py>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load and Inspect the Data:**\n",
    "- In the next cell, load the data using pandas and use `df.info()` to inspect its structure and identify problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and inspect\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('dirty_employee_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Perform Data Cleaning:**\n",
    "- In new cells, perform the following cleaning steps, inspecting the result after each one:\n",
    "  1.  **Standardise Department Names:** Use a dictionary and the `.replace()` method to merge 'HR'/'hr' into 'Human Resources' and 'Tech' into 'Technology'.\n",
    "  2.  **Fill Missing Salaries:** Use `.groupby()` and `.transform()` to fill missing salary values with the median salary of that employee's department.\n",
    "  3.  **Handle Missing Dates:** Remove any rows where the `start_date` is missing using `.dropna()`.\n",
    "  4.  **Correct Data Type:** Convert the `start_date` column to a proper datetime format using `pd.to_datetime()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clean data (can be split into multiple cells)\n",
    "department_map = {'HR': 'Human Resources', 'hr': 'Human Resources', 'IT': 'Technology', 'Tech': 'Technology'}\n",
    "df['department'] = df['department'].replace(department_map)\n",
    "\n",
    "df['salary'] = df.groupby('department')['salary'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df.dropna(subset=['start_date'], inplace=True)\n",
    "\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "\n",
    "print(\"Data after cleaning:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Create a Visualisation:**\n",
    "- In a new cell, use `sns.countplot()` to create a bar chart showing the number of employees in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='department', data=df, order=df['department'].value_counts().index)\n",
    "plt.title('Number of Employees per Department', fontsize=16)\n",
    "plt.xlabel('Number of Employees')\n",
    "plt.ylabel('Department')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Saving and Retrieving Your Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Save the Cleaned Data:**\n",
    "- In the final cell of your notebook, save the cleaned DataFrame to a new CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Download Your Files:**\n",
    "- In the Jupyter interface in your browser, first save the notebook (`File` -> `Save Notebook`).\n",
    "- Then, from your **local terminal**, use `scp` to download both your completed notebook and the new clean CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Practice Session\n",
    "- Congratulations! ðŸ¥³\n",
    "- You have successfully used a cloud server as an interactive data science workbench, from data acquisition to final analysis and retrieval of your work.\n",
    "- **Crucially, do not forget to terminate your EC2 instance** from the AWS Console to stop incurring charges.\n",
    "- To do this, go back to your SSH session running the Jupyter server, press `Ctrl+C` twice to stop it, then go to the AWS Console and terminate the `jupyter-ec2-instance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And that's all for today! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
