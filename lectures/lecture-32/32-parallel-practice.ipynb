{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTM 350 - Data Science Computing\n",
    "## Lecture 32: Parallel Computing & Docker\n",
    "**Author:** Danilo Freire (danilo.freire@emory.edu, Emory University)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In this practice session, you will apply your knowledge of parallel computing and containerisation. The goal is to move from writing simple serial code to creating scalable, parallel computations with Dask, and finally, packaging your entire workflow into a portable, reproducible Docker container.\n",
    "\n",
    "### Instructions\n",
    "1.  **Follow the sections sequentially.** The notebook is designed to build concepts progressively.\n",
    "2.  **Set up the environment first.** The initial steps are crucial for the subsequent tasks to run correctly.\n",
    "3.  **Complete the code in the designated cells.** For each question, fill in the necessary code to achieve the described outcome.\n",
    "4.  **Consult the lecture notes.** This practice directly relates to the concepts covered in lectures 26-30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup for Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92b285",
   "metadata": {},
   "source": [
    "### Task 1: Create a Conda Environment\n",
    "\n",
    "To ensure our work is reproducible, we must first define and create a consistent Python environment. This practice requires **Python 3.10** and several specific package versions.\n",
    "\n",
    "Write the **single terminal command** to create a new `conda` environment named `qtm350-parallel` that includes `python=3.10` and the following packages:\n",
    "- `dask-sql=2024.5.0`\n",
    "- `dask=2024.4.1`\n",
    "- `ipykernel=6.29.3`\n",
    "- `joblib=1.3.2`\n",
    "- `numpy=1.26.4`\n",
    "- `pandas=2.2.1`\n",
    "\n",
    "After creating it, remember to activate it and select it as the kernel for this notebook.\n",
    "\n",
    "You can either paste the bash/zsh command below, or run it from Python using the [subprocess](https://feigeek.com/how-to-activate-a-conda-environment-in-python-code.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287b1fa",
   "metadata": {},
   "source": [
    "### Task 2: Simulating Parallel Data Processing with `joblib`\n",
    "\n",
    "Imagine you have a list of 100 data records to process. Each record takes a small but non-trivial amount of time. Your task is to simulate this using `joblib` to see the benefits of parallelisation.\n",
    "\n",
    "1.  Define a function `process_record(record_id)` that simulates work by printing which record it's processing, then sleeps for 0.1 seconds (using `time.sleep(0.1)`) and returns `record_id * 2`.\n",
    "2.  Create a list of `record_ids` from 0 to 99.\n",
    "3.  Use `joblib.Parallel` to run this function on all record IDs using **4 cores** (`n_jobs=4`).\n",
    "4.  Measure and compare the total time taken for the parallel execution versus a simple serial `for` loop. You can use `%time` magic for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60216246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa3f2c",
   "metadata": {},
   "source": [
    "### Task 3: Large-Scale Matrix Operations with Dask Array\n",
    "\n",
    "In machine learning, you often perform operations on very large matrices. Dask is perfect for this.\n",
    "\n",
    "Your task:\n",
    "1.  Create a large Dask array `X` of random numbers with a shape of (20000, 5000), chunked into pieces of size (1000, 1000).\n",
    "2.  Perform a common linear algebra operation: compute the dot product of the transpose of `X` with `X` (i.e., `X.T @ X`).\n",
    "3.  Calculate the mean of the resulting matrix.\n",
    "4.  Execute the computation and print the final scalar result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70312898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbc320",
   "metadata": {},
   "source": [
    "### Task 4: Basic Aggregations with a Dask DataFrame\n",
    "\n",
    "This task focuses on performing a fundamental `groupby` aggregation on a larger-than-memory dataset.\n",
    "\n",
    "**Setup code (provided for you):**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Create a large sample pandas DataFrame\n",
    "n_rows = 1_000_000\n",
    "departments = ['Engineering', 'HR', 'Sales', 'Marketing']\n",
    "data = {\n",
    "    'department': np.random.choice(departments, n_rows),\n",
    "    'salary': np.random.randint(50000, 150000, n_rows),\n",
    "    'years_of_service': np.random.randint(1, 20, n_rows)\n",
    "}\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# Convert to a Dask DataFrame with 4 partitions\n",
    "df = dd.from_pandas(pdf, npartitions=4)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "Using the Dask DataFrame `df` created above, calculate the average salary for each department. Compute and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25cadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ddaed",
   "metadata": {},
   "source": [
    "### Task 5: Filtering and Aggregating with Dask-SQL\n",
    "\n",
    "This task uses the same employee dataset but introduces SQL for more expressive querying.\n",
    "\n",
    "**Setup code (provided for you):**\n",
    "```python\n",
    "from dask_sql import Context\n",
    "# The 'df' from the previous task is created again here to ensure this cell is self-contained.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "n_rows = 1_000_000\n",
    "departments = ['Engineering', 'HR', 'Sales', 'Marketing']\n",
    "data = {\n",
    "    'department': np.random.choice(departments, n_rows),\n",
    "    'salary': np.random.randint(50000, 150000, n_rows),\n",
    "    'years_of_service': np.random.randint(1, 20, n_rows)\n",
    "}\n",
    "pdf = pd.DataFrame(data)\n",
    "df = dd.from_pandas(pdf, npartitions=4)\n",
    "\n",
    "c = Context()\n",
    "c.create_table('employees', df)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "Write and execute a SQL query that finds the number of employees and their average years of service for each department, but **only include employees with a salary greater than $100,000**. Order the results by the count of employees in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22522b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37a7f7",
   "metadata": {},
   "source": [
    "## Part 3: Containerisation with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea919ab",
   "metadata": {},
   "source": [
    "### Task 6: A Dockerfile with Dependencies\n",
    "\n",
    "A \"Hello, World!\" is good, but a real script has dependencies. Let's create a Dockerfile for a Python script that uses `pandas`.\n",
    "\n",
    "Your task:\n",
    "Write a `Dockerfile` that:\n",
    "1.  Starts from the `python:3.10-slim` base image.\n",
    "2.  Uses `pip` to install `pandas`.\n",
    "3.  Sets the working directory to `/data`.\n",
    "4.  Copies a local script `generate_report.py` into the container.\n",
    "5.  Runs the script using `CMD`.\n",
    "\n",
    "*To test, create a local file `generate_report.py` that imports pandas, creates a simple DataFrame, and prints it. Sample code is provided below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e46f5",
   "metadata": {},
   "source": [
    "#### generate_report.py\n",
    "```python\n",
    "import pandas as pd\n",
    "data = {'Product': ['Apples', 'Oranges', 'Bananas'], 'Sales': [150, 200, 120]}\n",
    "df = pd.DataFrame(data)\n",
    "print('--- Sales Report ---')\n",
    "print(df)\n",
    "print('--------------------')\n",
    "```\n",
    "\n",
    "#### Dockerfile\n",
    "```dockerfile\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4376ce",
   "metadata": {},
   "source": [
    "### Task 7: A Reproducible Data Science Environment with Conda\n",
    "\n",
    "Using a `requirements.txt` or `environment.yml` file is the standard for reproducible environments. Let's build a Docker image using this best practice with Conda.\n",
    "\n",
    "Your task:\n",
    "1.  First, create a file named `environment.yml` locally. It should specify Python 3.10 and list `pandas`, `scikit-learn`, and `matplotlib` as dependencies.\n",
    "2.  Write a `Dockerfile` that:\n",
    "\n",
    "    a. Starts from a Conda-ready base image, like `continuumio/miniconda3`.\n",
    "\n",
    "    b. Sets a working directory, e.g., `/app`.\n",
    "\n",
    "    c. Copies the `environment.yml` file into the image.\n",
    "\n",
    "    d. Uses `conda env create -f environment.yml` to build the environment.\n",
    "\n",
    "    e. Sets the `CMD` to activate the conda environment and start a `bash` shell, allowing a user to run commands within the prepared environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605ea5b",
   "metadata": {},
   "source": [
    "#### environment.yml\n",
    "```yaml\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "#### Dockerfile\n",
    "```dockerfile\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Practice\n",
    "\n",
    "Congratulations! You've finished the practice! 🎉🥳"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtm350-parallel",
   "language": "python",
   "name": "qtm350-parallel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
